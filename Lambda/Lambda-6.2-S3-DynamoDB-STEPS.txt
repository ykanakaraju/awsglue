Automating the upload of CSV files from S3 to DynamoDB using Lambda
--------------------------------------------------------------------

  NOTE: Make sure you have all the entities created in the same region.
	-> us-east-1 (N.Virginia)

  1.  Create an IAM Role with all the required policies:
      (This role is used for all lambda example hense has wide access)

	Role Name: MyLambdaRole
	Policies:
		AmazonS3FullAccess
 		CloudWatchFullAccess
 		AmazonDynamoDBFullAccess
 		AmazonSESFullAccess 
		AmazonSNSFullAccess

  2. Create a DynamoDB Table

	-> Make sure the right region is selected.
		-> us-east-1 (N.Virginia)
	-> Table Name: "users"
	-> Partition-key: "id"

  3. Create a Lambda Function:

	-> Make sure the right region is selected.
		-> us-east-1 (N.Virginia)

	-> Click on 'Create Function'
		-> Function name: S3CSVDynamoDBPython
		-> Runtime: Runtime
		-> Architecture: keep the default

		-> Permissions: 
			-> Change default execution role:
			   -> Use an existing role : MyLambdaRole
		-> Click on 'Create Function'

	-> Add the Python Script in the 'Code' tab
		-> Check the script code given below (@ the end of this file)

	-> Setup a Test event to test the script
		-> Select 'Configure test event' from the dropdown.
		-> Event template:  's3 put'
		-> Change the following in the test object:
			
			s3 -> bucket -> name : "iquiz.lambda.csv.ddb"   (s3 bucket name)
			s3 -> object -> key  : "users.csv"  (file in the above bucket)

	-> Test the function and make sure the data is being added to DynamoDB table.


  4. Create an S3 bucket from which to upload data to DynamoDB

	s3://iquiz.lambda.csv.ddb

  5. Add a trigger to the Lambda function

	-> Click on "Add Trigger" button
	-> Trigger:  S3
		-> Bucket: iquiz.lambda.csv.ddb
		-> Event type: All object create events
		-> Recursive invocation: check the box
		-> Add

   6. Create events from the s3 bucket to be linked to the lambda
	-> Open the S3 bucket (iquiz.lambda.csv.ddb)
	-> Open 'Properties' tab
	-> Go to 'Event notifications' section
	-> Click on 'Create Notification' 
		Event name:  csv-upload
		Event types: Object creation -> Check 'All object create events'
		Destination: 
			Destination -> check 'Lambda function'
			Specify Lambda function -> Choose from your Lambda functions
			Select the Lambda from the dropdown. (S3CSVDynamoDBPython)

  7. Now, as you add the CSV files automatically the data will be added to DynamoDB tables;
		

   



Python Script:
--------------

#https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.get_object
#https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/dynamodb.html#table
#https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html     

import boto3
s3_client = boto3.client("s3")

dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('users')

def lambda_handler(event, context):
    bucket_name = event['Records'][0]['s3']['bucket']['name']
    s3_file_name = event['Records'][0]['s3']['object']['key']
    resp = s3_client.get_object(Bucket=bucket_name, Key=s3_file_name)
    data = resp['Body'].read().decode("utf-8")
    #print(data)
    
    users = data.split("\n")
    
    for user in users:
        print(user)
        
        user_data = user.split(",")
        
        table.put_item(
            Item = {
                "id" : user_data[0],
                "name": user_data[1],
                "gender": user_data[2],
                "age": user_data[3],
                "mobile": user_data[4]
            }
        )