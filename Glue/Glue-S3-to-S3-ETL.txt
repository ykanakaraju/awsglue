
 Example 1: ETL from one S3 bucket to another S3 bucket
 ------------------------------------------------------

 1. Load the required data to S3
	1.1 Create two folders in s3 to read from and write to:
		s3://iquiz.glue/movies-s3-to-s3/read
		s3://iquiz.glue/movies-s3-to-s3/write
	1.2 Load the dataset to the read folder
		Upload the movies-data.csv file to 's3://iquiz.glue/movies-s3-to-s3/read' folder

 2. Create an IAM Role (CTSGlueRole) for Glue with the following policies:			
	Service: Glue
	Policies:   
	  -> AmazonS3FullAccess
	  -> AmazonDynamoDBFullAccess
	  -> AWSGlueServiceRole
	  -> CloudWatchFullAccess

 3. Connect to AWS Glue service and create a Glue crawler:
	3.1  In the left panel of the Glue management console click Crawlers.
	3.2  Click the 'Create crawler' button.
	3.3  Give the crawler a name such as glue-s3-s3-crawler.
		-> Next
	3.4  Data source configuration
		    Is your data already mapped to Glue tables?: Not yet
		    Click on 'Add data source' button
			   Data source: S3
			   S3 path: s3://iquiz.glue/movies-s3-to-s3/read
			   Subsequent crawler runs: Crawl all sub-folders
			   -> Add an S3 data source
		    -> Next
	3.5  In Choose an IAM role 
			Create an IAM role
		       IAM role: AWSGlueServiceRole-S3toS3
			-> Next
		   
	3.6  Output configuration
			Add a new database called glue-s3-s3-db.
			Refresh and Select the database created
		Crawler schedule: On-demand
	3.7 -> Next -> Create crawler

 4. Run the crawler
	4.1  Goto Crawlers link to list all crawlers
	4.2  Select the crawler and Click 'Run' button.

		Notes:  
		------
		Once the data has been crawled, the crawler creates a metadata table from it. 
		You find the results from the Tables section of the Glue console. 
		The database that you created during the crawler setup is just an arbitrary way 
		of grouping the tables.

		Glue tables donâ€™t contain the data but only the instructions on how to access the data.

 5. Create the Glue job
 
	5.1 Select 'Jobs' menu option
	5.2 Jobs
		Create job: Spark script editor
		Options: Create a new script with boilerplate code
		-> Create
	5.3 Write the script in the editor (glue-s3-to-s3.py)
        -> Click on 'Save' button
	5.4 Open 'Job details' table	
		Name: glue-s3-s3-job
		IAM Role: CTSGlueRole   (created in step 2)
		Type: Spark
		Glue version: Glue 3.0
		Language: Python 3
		Worker type: G 1X
		Requested number of workers: 2	
		Job bookmark: Disable
		Job timeout: 30 
		Advanced properties
			Script filename: glue-s3-to-s3.py

 6. Save the changes and 'Run' the job

 7. Check the results
    7.1 To monitor the status click on 'Runs' tab
	7.1 Once the the job is complete, the results of the script are stored in output s3 folder
	    -> Output s3 folder: s3://iquiz.glue/movies-s3-to-s3/write/


==================================================
    pyspark script (glue-s3-to-s3.py)
==================================================

#########################################
### IMPORT LIBRARIES AND SET VARIABLES
#########################################

#Import python modules
from datetime import datetime

#Import pyspark modules
from pyspark.context import SparkContext
import pyspark.sql.functions as f

#Import glue modules
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job

#Initialize contexts and session
spark_context = SparkContext.getOrCreate()
glue_context = GlueContext(spark_context)
session = glue_context.spark_session

#Parameters
glue_db = "glue-s3-s3-db"
glue_tbl = "read"
s3_write_path = "s3://iquiz.glue/movies-s3-to-s3/write/"

#########################################
### EXTRACT (READ DATA)
#########################################

#Log starting time
dt_start = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
print("Start time:", dt_start)

#Read movie data to Glue dynamic frame
dynamic_frame_read = glue_context.create_dynamic_frame.from_catalog(database = glue_db, table_name = glue_tbl)

#Convert dynamic frame to data frame to use standard pyspark functions
data_frame = dynamic_frame_read.toDF()

#########################################
### TRANSFORM (MODIFY DATA)
#########################################

#Create a decade column from year
decade_col = f.floor(data_frame["year"]/10)*10
data_frame = data_frame.withColumn("decade", decade_col)

#Group by decade: Count movies, get average rating
data_frame_aggregated = data_frame.groupby("decade").agg(
    f.count(f.col("movie_title")).alias('movie_count'),
    f.mean(f.col("rating")).alias('rating_mean'),
)

#Sort by the number of movies per the decade
data_frame_aggregated = data_frame_aggregated.orderBy(f.desc("movie_count"))

#Print result table
#Note: Show function is an action. Actions force the execution of the data frame plan.
#With big data the slowdown would be significant without cacching.
data_frame_aggregated.show(10)

#########################################
### LOAD (WRITE DATA)
#########################################

#Create just 1 partition, because there is so little data
data_frame_aggregated = data_frame_aggregated.repartition(1)

#Convert back to dynamic frame
dynamic_frame_write = DynamicFrame.fromDF(data_frame_aggregated, glue_context, "dynamic_frame_write")

#Write data back to S3
glue_context.write_dynamic_frame.from_options(
    frame = dynamic_frame_write,
    connection_type = "s3",
    connection_options = {
        "path": s3_write_path,
        #Here you could create S3 prefixes according to a values in specified columns
        #"partitionKeys": ["decade"]
    },
    format = "csv"
)

#Log end time
dt_end = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
print("Start time:", dt_end)
