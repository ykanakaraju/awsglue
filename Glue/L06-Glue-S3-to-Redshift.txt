
  ===============================================================================
    Lab 6 - Load the data into Redshift table from an S3 bucket using AWS Glue.
   ==============================================================================
   
	* Create an S3 Bucket and load the data to be transferred
		(here we are using CSV files with header - users.csv, users2.csv, users3.csv)
	* Create a Redshift cluster and create a table.
	* Create a Glue classfier to read from CSV
	* Create a Glue crawler to crawl S3 bucket and Run it.
	* Create a Connection for Redshift
	* Create a Redshift Crawler and Run it.
	* Create a Glue Job and Run it
	* Verify the Results in Redshift Query Editor.
	

	
 1. Create an S3 bucket and load the data files into it.	
 -> Bucket: iquiz.demo1
   (here we are using CSV files with header - users.csv, users2.csv, users3.csv)

 2. Create a Redshift cluster and create a table.	
	2.1 Create a new Redshift cluster and wait until it comes 'Available' status
		-> Cluster configuration
		Cluster identifier: demo-cluster-1
		What are you planning to use this cluster for?  Free trial
		-> Database configurations
		Admin user name: demoadmin
		Admin user password: Demoadmin123
		
	**** Note down the "JDBC URL" which is needed later. ****
	
	2.2 Open the Query Editor v2 console
	2.3 Create a Schema (name: demo1)
		-> Option is available under "Create" dropdown in the Query Editor
	2.4 Create a table under this schema. 
		-> Execute the following query.
		-> Your table name should be <schema>.<table-name>

	create table demo1.users 
	(id bigint, name varchar(50), gender varchar(10), age bigint, phone bigint)
		
 3. Create a Glue classfier to read from CSV	
	3.1 Open the AWS Glue Console and click on 'Classifier' menu option
	3.2 Click on Add Classifier and fill the details:
		-> Classifier name : csv-classifier
		-> Classifier type and properties : CSV
		-> 'Create'
		
 4. Add a Glue database to be used with the crawlers.	
	4.1 Click on 'Databases' menu option
	4.2 Click on 'Add database' button 
	4.3 Provide a name and click on 'Create'
		-> Name: s3-redshift-db
		
 5. Create a Crawler to crawl the S3 bucket created in step 1.
	5.1 Click on 'Crawlers' menu option and click on 'Create Crawler' button
	5.2 Crawler Details
		-> Name: s3-crawler 
		-> 'Next'
	5.3 Data source configuration
		-> Is your data already mapped to Glue tables?  Not yet		
		-> Data sources -> Add a data source
		-> Data source: S3
		-> Location of S3 data: In this account
		-> S3 path: s3://iquiz.demo1
		-> Subsequent crawler runs: Crawl all sub-folders
		-> Add an S3 data source
		-> Next
	5.4 Configure security settings : Create an IAM role
		IAM Role: AWSGlueServiceRole-CSVRedshift
		-> Next

	5.5 Set output and scheduling
		-> Target database : s3-redshift-db   (created in step 4)
		-> Crawler schedule : On demand
		-> Next

	5.6 Review and create
		-> Create crawler
		
 6. Run the Crawler
	6.1 Select the crawler just created and click on 'Run' button
	6.2 Observe the 'state' while the crawler is running. 
		==> Wait until the 'state' turns to 'Ready'	
		
 7. Create a connection to connect to Redshift
	7.1 Click the "Connections" menu option
	7.2  Click on "Create connection" button and add the following details:
		-> Name: redshift-connection
		-> Connection type : Amazon Redshift
		-> Database instances : demo-cluster-1 (created in step 2)
		-> Database name: dev
		-> Username: demoadmin
		-> Password: Demoadmin123
		
 8. Create another Crawler to crawl Redshift table.
	=> Follow the same steps as mentioned in 5 & 6 with the following changes:
		Name: redshift-crawler
		Under Add data source
			Data source: JDBC
			Connection: redshift-connection  (Created in step 7)
			Include path: dev/demo1/users  (<database>/<schema>/<table>)
			IAM Role: AWSGlueServiceRole-CSVRedshift (same as previous step)
			
 9. Create a Glue Job and run it	
	9.1 Open Glue Console and select 'Jobs' option
	9.2 Jobs
		Create job: Visual with a source and target
		Source: Amazon S3
		Target: Amazon Redshift
		-> Create
	9.3 Setup the Graph
		9.3.1 Click on the 'S3 bucket' (source) on the graph & set the following:
			-> S3 source type: Data Catalog table
			-> Database: s3-redshift-db
			-> Table: iquiz_demo1
		9.3.2 Click on the 'Redshift Cluster' (target) on the graph & set the following:
			-> Database: s3-redshift-db
			-> Table: dev_demo1_users
		9.3.3 Click on the 'Apply mappings' (transform) on the graph and make sure the mapping are correct.
			
		9.3.4 Go to Job details tab
			-> Name: s3-redshift-job
			-> IAM Role: AWSGlueServiceRole-CSVRedshift
			-> Type: Spark
			-> Glue version: keep default (i.e Glue 3.0)
			-> Language: Python 3
			-> Worker type: keep default (i.e G 1X)
			-> Requested number of workers: 2
			-> Generate job insights: uncheck
			-> Job bookmark: Disable
			-> Job timeout (minutes): 30		
	9.4 Click on 'Save'
	9.5 Click on 'Run' button
		
