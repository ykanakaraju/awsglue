
 ==========================================================
  Lab 7: Backup DynamoDB Tables in S3 (in Parquet format)
 ==========================================================

  For the end-to-end process, S3, Glue, DynamoDB, and Athena will be utilized and will follow these steps:

  1. Crawl the DynamoDB table with Glue to register the metadata of our table with the Glue Data Catalog. 
     Once thatâ€™s done, we can query the table with Athena.

  2. Create a Glue job for copying table contents into S3 in parquet format.

  3. Crawl the S3 bucket with Glue crawler and validate it using Athena.	


  Steps
  ------

	 1. Create an IAM Role with required Access

		Name: CTSGlueRole
		Policies: 
			AmazonS3FullAccess
			CloudWatchFullAccess
	 		AmazonDynamoDBFullAccess
	 		AWSGlueServiceRole
	 		AmazonRedshiftFullAccess

	 2. Make sure that you have a DynamoDB table with data.  (table used here : users)

	 3. Create an AWS Glue Crawler for DynamoDB

		3.1 Add information about your crawler
			Crawler name: CTS Glue DynamoDB to S3

		3.2 Specify crawler source type
			Crawler source type: Data stores
			Repeat crawls of S3 data stores: Crawl all folders

		3.3 Add a data store
			Choose a data store: DynamoDB
			Table name: users (you need to have this table in DynamoDB)
			Scanning rate: 0.5
			Add another data store: No
			Choose an IAM role
				Choose an existing IAM role: CTSGlueRole

		3.4 Create a schedule for this crawler
			Frequency: Run on Demand

		3.5 Configure the crawler's output
			-> Database		
			   -> Add Database
				-> Database name: glue-ddb-s3-db
				-> Create

		3.6 Review and click on 'Finish'

	 4. Run Crawler
		4.1 Select the crawler and click on 'Run Crawler' button

		-> Once the crawler has finished running, navigate to the Tables under 
		   Data Catalog to ensure the table is created and the metadata looks 
	       accurate such as the DynamoDB table schema and row count
			
	 5. Create a Glue Job
		5.1 Configure the job properties
			Name: glue-dynamodb-s3-job
			IAM role: CTSGlueRole
			Type: Spark
			Glue version: Spark 2.4, Python 3
			This job runs: A proposed script generated by AWS Glue
			Script file name: glue-dynamodb-s3-job

		5.2 Choose a data source
			   -> Select DynamoDB table   (table: users - database: glue-ddb-s3-db)

		5.3 Choose a transform type
			   -> Change schema

		5.4 Choose a data target
			Create tables in your data target
			Data store: Amazon S3
			Format: Parquet
			Target path: s3://iquiz.glue/dynamodb-backup

		5.5 Output Schema Definition
				Review mappings and click on "Save Job and edit script"

	    5.6 Review the code and click on "Run Job"
			
	   
	  6. Create a Glue Crawler for S3
		6.1 Add information about your crawler
			Crawler name: ddb-s3-view

		6.2 Specify crawler source type
			Crawler source type: Data stores
			Repeat crawls of S3 data stores : Crawl all folders

		6.3 Add a data store
			Choose a data store: S3
			Crawl data in: Specified path
			Include path: s3://iquiz.glue/dynamodb-backup

		6.4 Add another data store : No

		6.5 Choose an IAM role
			-> Choose an existing IAM role
				-> CTSGlueRole

		6.6 Create a schedule for this crawler
			Frequency: Run on demand

		6.7 Configure the crawler's output
			Database: dynamodb-backup   (this is created in a prior step)
			-> Next
			-> Finish
			
	  7. Run Crawler
			Select the crawler and click on 'Run Crawler' button		
					 

	  8. Querying via Athena
		Navigate to AWS Athena and Connect to the Query editor 
		Select AWSDataCatalog for the Data Source on the left menu
		Choose the database in which the crawler will store the data.
		Use the query editor to run queries against the table created by crawler
		Test functionality by running the following command to ensure the format was extracted properly: 
		     Query: SELECT * FROM <db>.<table> LIMIT 10; 

		NOTE:
		Before you run your first query, you need to set up a query result location in Amazon S3.

		-> Settings -> manage -> Enter an S3 path where you want to store the results













