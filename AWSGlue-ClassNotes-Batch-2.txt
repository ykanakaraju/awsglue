
  Agenda - AWS Glue
  ----------------------------   
   1. PySpark & SparkSQL
   2. AWS Basics & AWS Infra		
   3. Amazon S3				
   4. AWS Lambda			
   5. AWS Glue				
   6. AWS Athena 			
   7. Project (CDC)


  Course Materials
  ----------------
    -> PDF Presentations
    -> Code Modules, Instructions, Scripts, Notebooks, Datasets
    -> Class Notes
    -> Github: https://github.com/ykanakaraju/awsglue 


  Spark
  ----- 

   -> Spark is an in-memory distributed computing framework for performing big data analytics.

   -> Spark is written in SCALA language

   -> Spark is a polyglot
	-> Scala, Java, Python, R (and SQL)

   -> Spark has unified stack for data analytics

   -> Spark APIs
	-> Spark Core API  : Low-level API. Uses RDDs
	-> Spark SQL	   : Structured/semi-structured data processing (Batch)
	-> Spark Streaming : DStreams API, Structured streaming
	-> Spark MLLib	   : Predictive analytics using ML
	-> Spark GraphX	   : Graph Processing   

    -> Spark can run on multiple cluster managers
	-> local, Spark standalone, YARN, Mesos, Kubernetes 


  Spark Architecture
  ------------------

    	1. Cluster Manager (CM)
		-> Applications are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the 'SparkContext' object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		-> Where to run the driver process
		1. Client : default, driver runs on the client. 
		2. Cluster: driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks run the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 

	
  Getting started with Spark on Databricks
  ----------------------------------------
   ** Databricks Community Edition (free edition)
 		
	Signup: https://www.databricks.com/try-databricks
		-> Click on Try Databricks link @ top-right corner
		
		Click on "Click here" link (@ bottom-right corner)
		==> Looking for Databricks Community Edition? Click here

	Login: https://community.cloud.databricks.com/login.html

	Downloading a file from Databricks
	----------------------------------
		/FileStore/<FILEPATH>
		https://community.cloud.databricks.com/files/<FILEPATH>?o=1072576993312365

		Example:
		dbfs:/FileStore/output/wc/part-00000
		https://community.cloud.databricks.com/files/output/wc/part-00000?o=1072576993312365

	Enabling DBFS File browser
	--------------------------
	<your account (top-right)> -> Settings -> Advanced -> Other -> DBFS File Browser (enable it)


  RDD (Resilient Distributed Dataset)
  -----------------------------------
	-> RDD is the fundamental data abstraction of Spark

	-> RDD is a collection of distributed in-memory partitions.
	    -> Each partition is a collection of objects of some type.

	-> RDDs are immutable

	-> RDDs are lazily evaluated
	   -> Transformations does not cause execution.
	   -> Action commands trigger execution. 


   Creating RDDs
   -------------
    Three ways:

	1. Creating an RDD from external data file

		rdd1 = sc.textFile(<dataPath>, 4)

	2. Creating an RDD from programmatic data

		rdd1 = sc.parallelize([2,1,3,2,4,3,5,4,6,7,5,6,7,6,8,8,9,0], 2)

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*10)


  RDD Operations
  --------------

    Two types of operations

	1. Transformations
		-> Transformations return RDDs
		-> Transformations does not cause execution of RDDs
		-> Cause lineage DAGs to be created

	2. Actions
		-> Triggers execution of RDDs
		-> Produces output by sending a Job to the cluster


  RDD Lineage DAG
  ---------------
    Driver maintains a Lineage DAG for every RDD.
    Lineage DAG is a heirarchy of dependencies of RDDs all the way starting from the very first RDD.	
    Lineage DAG is a logical plan on how to create the RDD partitions.

	rdd_file = sc.textFile(file_path, 4)
	Lineage DAG: (4) rdd_file -> sc.textFile on dbfs:/FileStore/wordcount.txt

	rdd_words = rdd_file.flatMap(lambda x: x.split())
	Lineage DAG: (4) rdd_words -> rdd_file.flatMap -> sc.textFile on dbfs:/FileStore/wordcount.txt

	rdd_pairs = rdd_words.map(lambda x: (x, 1))
	Lineage DAG: (4) rdd_pairs -> rdd_words.map -> rdd_file.flatMap -> sc.textFile

	rdd_wordcount = rdd_pairs.reduceByKey(lambda x, y: x + y)
	Lineage DAG: (4) rdd_wordcount -> rdd_pairs.reduceByKey -> rdd_words.map -> rdd_file.flatMap -> sc.textFile


  Spark DAG Scheduler
  -------------------

	Application  (SparkContext represents an application)
	|
	|--> Jobs  (each action command launches a Job)
		|
		|--> Stages (each wide transformation in the RDD DAG of a job causes stage transition)
			|
			|--> Tasks (each task has one or more transformations that run in parallel)

  
   Spark SQL
   =========

    -> High Level API built on top of Spark Core
    -> Structured data processing API

	File Formats : Parquet (default), ORC, JSON, CSV (delimited text), Text
	JDBC Format  : RDBMS, NoSQL
	Hive Format  : Hive Warehouse


  SparkSession
  ------------	
	-> Starting point of execution in Spark SQL functionality
	-> Represents a user session (SparkSession) running inside an application (SparkContext)
	-> Each SparkSession can have its own configuration

	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local") \
    		.getOrCreate()	


   DataFrame (DF)
   --------------
	-> Main data abstraction of Spark SQL
	-> Is a collection of distributed in-memory partitions
	-> Immutable
	-> Lazily evaluated

	-> DataFrame is a collection of "Row" objects.

	-> DataFrame contains two components:
		-> Data    : Collection of 'Row' objects
		-> Schema  : StructType object

			StructType(
			   [
				StructField('age', LongType(), True), 
				StructField('gender', StringType(), True), 
				StructField('name', StringType(), True), 
				StructField('phone', StringType(), True), 
				StructField('userid', LongType(), True)
			   ]
			)


   Basic steps in creating a Spark SQL Application
   -----------------------------------------------

	1. Read/load data from some data-source into a DataFrame

 		inputPath = "E:\\PySpark\\data\\users.json"
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

		df1.show()
		df1.printSchema()


	2. Transform the DF using DF transformation methods or using SQL

	        Using DF transformation methods
		--------------------------------
		df2 = df1.select("userid", "name", "age", "gender", "phone") \
        		.where("age is not null") \
        		.orderBy("gender", "age") \
        		.groupBy("age").count() \
        		.limit(4)


		Using SQL
		---------
		df1.createOrReplaceTempView("users")
		spark.catalog.listTables()

		qry = """select age, count(*) as count
			from users
			where age is not null
			group by age
			order by age
			limit 4"""
					
		df3 = spark.sql(qry)
		df3.show()


	3. Save the dataframe into some external destination (such as files/databases/hive etc)

		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)

		df3.write.json(outputPath, mode="overwrite")


  Save Modes
  ----------
    -> Control the behaviour when saving a DF into an existing directory.

	1. errorIfExists (default)
	2. ignore
	3. append
	4. overwrite

	df3.write.mode("overwrite").json(outputPath)
	df3.write.json(outputPath, mode="overwrite")


  LocalTempViews & GlobalTempViews
  --------------------------------
	LocalTempView 
	   -> Local to a specific SparkSession
	   -> Created using createOrReplaceTempView command
		df1.createOrReplaceTempView("users")


	GlobalTempView
	   -> Can be accessed from multiple SparkSessions within the application
	   -> Tied to "global_temp" database
	   -> Created using createOrReplaceGlobalTempView command
		df1.createOrReplaceGlobalTempView("gusers")






