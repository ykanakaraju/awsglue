
  Agenda - AWS Glue
  ----------------------------   
   1. PySpark & SparkSQL
   2. AWS Basics & AWS Infra		
   3. Amazon S3				
   4. AWS Lambda			
   5. AWS Glue				
   6. AWS Athena 			
   7. Project (CDC)


  Course Materials
  ----------------
    -> PDF Presentations
    -> Code Modules, Instructions, Scripts, Notebooks, Datasets
    -> Class Notes
    -> Github: https://github.com/ykanakaraju/awsglue 


  Spark
  ----- 

   -> Spark is an in-memory distributed computing framework for performing big data analytics.

   -> Spark is written in SCALA language

   -> Spark is a polyglot
	-> Scala, Java, Python, R (and SQL)

   -> Spark has unified stack for data analytics

   -> Spark APIs
	-> Spark Core API  : Low-level API. Uses RDDs
	-> Spark SQL	   : Structured/semi-structured data processing (Batch)
	-> Spark Streaming : DStreams API, Structured streaming
	-> Spark MLLib	   : Predictive analytics using ML
	-> Spark GraphX	   : Graph Processing   

    -> Spark can run on multiple cluster managers
	-> local, Spark standalone, YARN, Mesos, Kubernetes 


  Spark Architecture
  ------------------

    	1. Cluster Manager (CM)
		-> Applications are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the 'SparkContext' object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		-> Where to run the driver process
		1. Client : default, driver runs on the client. 
		2. Cluster: driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks run the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 

	
  Getting started with Spark on Databricks
  ----------------------------------------
   ** Databricks Community Edition (free edition)
 		
	Signup: https://www.databricks.com/try-databricks
		-> Click on Try Databricks link @ top-right corner
		
		Click on "Click here" link (@ bottom-right corner)
		==> Looking for Databricks Community Edition? Click here

	Login: https://community.cloud.databricks.com/login.html

	Downloading a file from Databricks
	----------------------------------
		/FileStore/<FILEPATH>
		https://community.cloud.databricks.com/files/<FILEPATH>?o=1072576993312365

		Example:
		dbfs:/FileStore/output/wc/part-00000
		https://community.cloud.databricks.com/files/output/wc/part-00000?o=1072576993312365

	Enabling DBFS File browser
	--------------------------
	<your account (top-right)> -> Settings -> Advanced -> Other -> DBFS File Browser (enable it)


  RDD (Resilient Distributed Dataset)
  -----------------------------------
	-> RDD is the fundamental data abstraction of Spark

	-> RDD is a collection of distributed in-memory partitions.
	    -> Each partition is a collection of objects of some type.

	-> RDDs are immutable

	-> RDDs are lazily evaluated
	   -> Transformations does not cause execution.
	   -> Action commands trigger execution. 


  Creating RDDs
  -------------
    Three ways:

	1. Creating an RDD from external data file

		rdd1 = sc.textFile(<dataPath>, 4)

	2. Creating an RDD from programmatic data

		rdd1 = sc.parallelize([2,1,3,2,4,3,5,4,6,7,5,6,7,6,8,8,9,0], 2)

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*10)


  RDD Operations
  --------------

    Two types of operations

	1. Transformations
		-> Transformations return RDDs
		-> Transformations does not cause execution of RDDs
		-> Cause lineage DAGs to be created

	2. Actions
		-> Triggers execution of RDDs
		-> Produces output by sending a Job to the cluster


  RDD Lineage DAG
  ---------------
    Driver maintains a Lineage DAG for every RDD.
    Lineage DAG is a heirarchy of dependencies of RDDs all the way starting from the very first RDD.	
    Lineage DAG is a logical plan on how to create the RDD partitions.

	rdd_file = sc.textFile(file_path, 4)
	Lineage DAG: (4) rdd_file -> sc.textFile on dbfs:/FileStore/wordcount.txt

	rdd_words = rdd_file.flatMap(lambda x: x.split())
	Lineage DAG: (4) rdd_words -> rdd_file.flatMap -> sc.textFile on dbfs:/FileStore/wordcount.txt

	rdd_pairs = rdd_words.map(lambda x: (x, 1))
	Lineage DAG: (4) rdd_pairs -> rdd_words.map -> rdd_file.flatMap -> sc.textFile

	rdd_wordcount = rdd_pairs.reduceByKey(lambda x, y: x + y)
	Lineage DAG: (4) rdd_wordcount -> rdd_pairs.reduceByKey -> rdd_words.map -> rdd_file.flatMap -> sc.textFile


  Spark DAG Scheduler
  -------------------

	Application  (SparkContext represents an application)
	|
	|--> Jobs  (each action command launches a Job)
		|
		|--> Stages (each wide transformation in the RDD DAG of a job causes stage transition)
			|
			|--> Tasks (each task has one or more transformations that run in parallel)

  
  Spark SQL
  =========

    -> High Level API built on top of Spark Core
    -> Structured data processing API

	File Formats : Parquet (default), ORC, JSON, CSV (delimited text), Text
	JDBC Format  : RDBMS, NoSQL
	Hive Format  : Hive Warehouse


  SparkSession
  ------------	
	-> Starting point of execution in Spark SQL functionality
	-> Represents a user session (SparkSession) running inside an application (SparkContext)
	-> Each SparkSession can have its own configuration

	spark = SparkSession \
    		.builder \
    		.appName("Basic Dataframe Operations") \
    		.config("spark.master", "local") \
    		.getOrCreate()	


   DataFrame (DF)
   --------------
	-> Main data abstraction of Spark SQL
	-> Is a collection of distributed in-memory partitions
	-> Immutable
	-> Lazily evaluated

	-> DataFrame is a collection of "Row" objects.

	-> DataFrame contains two components:
		-> Data    : Collection of 'Row' objects
		-> Schema  : StructType object

			StructType(
			   [
				StructField('age', LongType(), True), 
				StructField('gender', StringType(), True), 
				StructField('name', StringType(), True), 
				StructField('phone', StringType(), True), 
				StructField('userid', LongType(), True)
			   ]
			)


   Basic steps in creating a Spark SQL Application
   -----------------------------------------------

	1. Read/load data from some data-source into a DataFrame

 		inputPath = "E:\\PySpark\\data\\users.json"
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

		df1.show()
		df1.printSchema()


	2. Transform the DF using DF transformation methods or using SQL

	        Using DF transformation methods
		--------------------------------
		df2 = df1.select("userid", "name", "age", "gender", "phone") \
        		.where("age is not null") \
        		.orderBy("gender", "age") \
        		.groupBy("age").count() \
        		.limit(4)


		Using SQL
		---------
		df1.createOrReplaceTempView("users")
		spark.catalog.listTables()

		qry = """select age, count(*) as count
			from users
			where age is not null
			group by age
			order by age
			limit 4"""
					
		df3 = spark.sql(qry)
		df3.show()


	3. Save the dataframe into some external destination (such as files/databases/hive etc)

		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)

		df3.write.json(outputPath, mode="overwrite")


  Save Modes
  ----------
    -> Control the behaviour when saving a DF into an existing directory.

	1. errorIfExists (default)
	2. ignore
	3. append
	4. overwrite

	df3.write.mode("overwrite").json(outputPath)
	df3.write.json(outputPath, mode="overwrite")


  LocalTempViews & GlobalTempViews
  --------------------------------
	LocalTempView 
	   -> Local to a specific SparkSession
	   -> Created using createOrReplaceTempView command
		df1.createOrReplaceTempView("users")


	GlobalTempView
	   -> Can be accessed from multiple SparkSessions within the application
	   -> Tied to "global_temp" database
	   -> Created using createOrReplaceGlobalTempView command
		df1.createOrReplaceGlobalTempView("gusers")


  Working with different file formats
  -----------------------------------

   JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	write
		df3.write.format("json").save(outputPath)
		df3.write.save(outputPath, format="json")
		df3.write.json(outputPath)	

   Parquet (default)
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	write
		df3.write.format("parquet").save(outputPath)
		df3.write.save(outputPath, format="parquet")
		df3.write.parquet(outputPath)


   ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	write
		df3.write.format("orc").save(outputPath)
		df3.write.save(outputPath, format="orc")
		df3.write.orc(outputPath)


   CSV (delimited text)

	read
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.format("csv").load(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
		df3.write.format("csv").save(outputPath, header=True)
		df2.write.csv(outputPath, header=True)
		df2.write.csv(outputPath, header=True, sep="|", mode="overwrite")

   Text
	read
		df1 = spark.read.text(inputPath)
		=> df1 will have one columns called 'value' of 'string' type

	write
		df1.write.text(outputPath)
		=> You can only save a DF with a single text column in 'text' format.



 DF Transformations
 ------------------

 1. select

	df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")

	df2 = df1.select(col("ORIGIN_COUNTRY_NAME").alias("origin"),
                 column("DEST_COUNTRY_NAME").alias("destination"),
                 expr("count").cast("int"),
                 expr("count + 10 as newCount"),
                 expr("count > 200 as highFrequency"),
                 expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"))


  2. where / filter

	df3 = df2.where("highFrequency = true and destination = 'United States'")
	df3 = df2.filter("highFrequency = true and destination = 'United States'")

	df3 = df2.where( col("count") > 100 )

	df3.show(5)


  3. orderBy / sort

	df3 = df2.orderBy("count", "origin")
	df3 = df2.orderBy(desc("count"), asc("origin"))


  4. groupBy  => returns a "GroupedData" object
		 apply some aggregation method to return a DataFrame


	df3 = df2.groupBy("highFrequency", "domestic").count()
	df3 = df2.groupBy("highFrequency", "domestic").sum("count")
	df3 = df2.groupBy("highFrequency", "domestic").avg("count")
	df3 = df2.groupBy("highFrequency", "domestic").max("count")

	df3 = df2.groupBy("highFrequency", "domestic") \
        	  .agg( count("count").alias("count"),
              		sum("count").alias("sum"),
              		max("count").alias("max"),
              		round(avg("count"), 2).alias("avg")
		   )


  5. limit

	df2 = df1.limit(10)


  6. selectExpr

	   df2 = df1.selectExpr("ORIGIN_COUNTRY_NAME as origin",
                 		"DEST_COUNTRY_NAME as destination",
                 		"count",
                 		"count + 10 as newCount",
                 		"count > 200 as highFrequency",
                 		"ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")


  7. withColumn 

	df3 = df1.withColumn("newCount", col("count") + 10) \
        	.withColumn("highFrequency", expr("count > 200")) \
        	.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
        	.withColumn("count", col("count").cast("int")) \
        	.withColumn("country", lit("India") )

	df3 = df2.withColumn("ageGroup", when(col("age") < 13, "child")
                                 	.when(col("age") < 20, "teenager")
                                 	.when(col("age") < 60, "adult")
                                 	.otherwise("senior"))

  8. withColumnRenamed

	df3 = df1.withColumn("newCount", col("count") + 10) \
		.withColumn("highFrequency", expr("count > 200")) \
		.withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME")) \
		.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")


  9. drop	=> excludes the specified columns of the input DF in the output DF.

	df3 = df2.drop("newCount", "highFrequency")
	df3.show()


  10. dropDuplicates   => drops the duplicate rows

	listUsers = [(1, "Raju", 5),
				 (1, "Raju", 5),
				 (3, "Raju", 5),
				 (4, "Raghu", 35),
				 (4, "Raghu", 35),
				 (6, "Raghu", 35),
				 (7, "Ravi", 70)]

	userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])
	userDf.show()

	userDf.dropDuplicates().show()
	userDf.dropDuplicates(["name", "age"]).show()



  11. distinct	 => returns distinct rows of the DF

	userDf.distinct().show()


	Q: How many UNIQUE values are there in DEST_COUNTRY_NAME column?
	-----------------------------------------------------------------
	df1.select("DEST_COUNTRY_NAME").distinct().count()
	df1.dropDuplicates(["DEST_COUNTRY_NAME"]).count()


  12. union

	df2 = df1.where("count > 1000")
	df2.show()
	df2.count()   # 14 rows
	df2.rdd.getNumPartitions()


  13. repartition

	df2 = df1.repartition(6)
	df2.rdd.getNumPartitions()

	df3 = df2.repartition(3)
	df3.rdd.getNumPartitions()

	df4 = df2.repartition(3, col("DEST_COUNTRY_NAME"))
	df4.rdd.getNumPartitions()

	df5 = df2.repartition(col("DEST_COUNTRY_NAME"))
	df5.rdd.getNumPartitions()


  14. coalesce

	df6 = df5.coalesce(3)
	df6.rdd.getNumPartitions()


  15. join   -> discussed separatly



